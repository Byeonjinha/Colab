{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tacotron2(tutorial).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcwCSe1zy4c5MNyVLF9ns2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bafe492a22c4a0f9730f13b687aa05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e69301d809c43f59a4a6d43052bd71a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_81995bae1a554f56ba2e3c858fd4fb8e",
              "IPY_MODEL_be4f63416f0b4e94afa6d71ed18331fb",
              "IPY_MODEL_32a140eeb496467fbc664d3f6cae284b"
            ]
          }
        },
        "3e69301d809c43f59a4a6d43052bd71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81995bae1a554f56ba2e3c858fd4fb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fada897fe37d40f58f4bf2578d90437d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb41f4b97911413aa85e626b742c6eb7"
          }
        },
        "be4f63416f0b4e94afa6d71ed18331fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4f802854fd41472c916bb31b5cd66d7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2748572632,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2748572632,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d65aa0567734eb385dc56e46b0a89a8"
          }
        },
        "32a140eeb496467fbc664d3f6cae284b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e4ccd3d74104de98ad6ba094326c7ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.56G/2.56G [01:12&lt;00:00, 35.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2397e68a87644dedbd01fe096e8d9b3a"
          }
        },
        "fada897fe37d40f58f4bf2578d90437d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb41f4b97911413aa85e626b742c6eb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f802854fd41472c916bb31b5cd66d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d65aa0567734eb385dc56e46b0a89a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e4ccd3d74104de98ad6ba094326c7ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2397e68a87644dedbd01fe096e8d9b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Byeonjinha/Colab/blob/main/%EC%9E%AC%EB%B0%8C%EB%8A%94%20%EA%B1%B0%20/Tacotron2(tutorial).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bokOujwqnJZA",
        "outputId": "be9adca2-6330-4345-c4d0-5e63700c83ca"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 13.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:38tcmalloc: large alloc 1147494400 bytes == 0x55c8e0f14000 @  0x7f03f007c615 0x55c8a78e74cc 0x55c8a79c747a 0x55c8a78ea2ed 0x55c8a79dbe1d 0x55c8a795de99 0x55c8a79589ee 0x55c8a78ebbda 0x55c8a795dd00 0x55c8a79589ee 0x55c8a78ebbda 0x55c8a795a737 0x55c8a79dcc66 0x55c8a7959daf 0x55c8a79dcc66 0x55c8a7959daf 0x55c8a79dcc66 0x55c8a7959daf 0x55c8a78ec039 0x55c8a792f409 0x55c8a78eac52 0x55c8a795dc25 0x55c8a79589ee 0x55c8a78ebbda 0x55c8a795a737 0x55c8a79589ee 0x55c8a78ebbda 0x55c8a7959915 0x55c8a78ebafa 0x55c8a7959c0d 0x55c8a79589ee\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0 torchaudio-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjfaYaven-C0",
        "outputId": "1fdaf581-65b6-4f94-e590-77555bd66e26"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from math import sqrt\n",
        "\n",
        "print('Import 완료')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp0K47e1pAdr",
        "outputId": "58df43eb-ecb7-4c53-f026-adb8453db730"
      },
      "source": [
        "class HParams():\n",
        "    def __init__(self):\n",
        "        self.n_mel_channels = 80\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        self.symbols_embedding_dim=512\n",
        "\n",
        "        # Encoder parameters\n",
        "        self.encoder_kernel_size=5\n",
        "        self.encoder_n_convolutions=3\n",
        "        self.encoder_embedding_dim=512\n",
        "\n",
        "        # Decoder parameters\n",
        "        self.n_frames_per_step=1  # currently only 1 is supported\n",
        "        self.decoder_rnn_dim=1024\n",
        "        self.prenet_dim=256\n",
        "        self.max_decoder_steps=1000\n",
        "        self.gate_threshold=0.5\n",
        "        self.p_attention_dropout=0.1\n",
        "        self.p_decoder_dropout=0.1\n",
        "\n",
        "        # Attention parameters\n",
        "        self.attention_rnn_dim=1024\n",
        "        self.attention_dim=128\n",
        "\n",
        "        # Location Layer parameters\n",
        "        self.attention_location_n_filters=32\n",
        "        self.attention_location_kernel_size=31\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        self.postnet_embedding_dim=512\n",
        "        self.postnet_kernel_size=5\n",
        "        self.postnet_n_convolutions=5\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        self.use_saved_learning_rate=False\n",
        "        self.learning_rate=1e-3\n",
        "        self.weight_decay=1e-6\n",
        "        self.grad_clip_thresh=1.0\n",
        "        self.batch_size=4\n",
        "        self.mask_padding=True  # set model's padded outputs to padded values\n",
        "    \n",
        "    \n",
        "hparams = HParams()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('GPU를 사용할 준비가 되었습니다.')\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print('CPU 모드입니다. GPU 설정으로 변경해주세요.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU를 사용할 준비가 되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "0bafe492a22c4a0f9730f13b687aa05c",
            "3e69301d809c43f59a4a6d43052bd71a",
            "81995bae1a554f56ba2e3c858fd4fb8e",
            "be4f63416f0b4e94afa6d71ed18331fb",
            "32a140eeb496467fbc664d3f6cae284b",
            "fada897fe37d40f58f4bf2578d90437d",
            "fb41f4b97911413aa85e626b742c6eb7",
            "4f802854fd41472c916bb31b5cd66d7f",
            "9d65aa0567734eb385dc56e46b0a89a8",
            "1e4ccd3d74104de98ad6ba094326c7ea",
            "2397e68a87644dedbd01fe096e8d9b3a"
          ]
        },
        "id": "vVIsVy8OpA9V",
        "outputId": "678e4800-2bbf-40e8-b3ee-3bb5d55ea8d1"
      },
      "source": [
        "LJSpeech_url = 'https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2'\n",
        "train_dataset = torchaudio.datasets.LJSPEECH(\"\", url=LJSpeech_url, download=True)\n",
        "\n",
        "print('Download 완료')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bafe492a22c4a0f9730f13b687aa05c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/2.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLSlJv6cpFts",
        "outputId": "6307d8a4-2013-4411-82f3-b3baa56afba0"
      },
      "source": [
        "vocab = \" abcdefghijklmnopqrstuvwxyz'.?\"  # P: Padding, E: EOS.\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def text_normalize(text):\n",
        "    text = ''.join(char for char in unicodedata.normalize('NFD', text)\n",
        "                   if unicodedata.category(char) != 'Mn')  # Strip accents\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^{}]\".format(vocab), \" \", text)\n",
        "    text = re.sub(\"[ ]+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "class Collate():\n",
        "  def __init__(self):\n",
        "    self.wav_to_mel = torchaudio.transforms.MelSpectrogram(sample_rate=22050, n_mels=80, win_length=1024, hop_length=256, f_min=0.0, f_max=8000.0, n_fft=1024)\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    # batch: N_batch * [wav, sample_rate, text, text_normalized]\n",
        "    mel_list = []\n",
        "    for data in batch:\n",
        "      wav = data[0]\n",
        "      mel_list.append(self.wav_to_mel(wav).squeeze())\n",
        "    input_lengths, ids_sorted_decreasing = torch.sort(torch.LongTensor([len(data[3]) for data in batch]), dim=0, descending=True)\n",
        "    mel_lengths, ids_sorted_mel = torch.sort(torch.LongTensor([mel.shape[1] for mel in mel_list]), dim=0, descending=True)\n",
        "\n",
        "    max_input_len = input_lengths[0]\n",
        "    max_target_len = mel_lengths[0]\n",
        "\n",
        "    text_padded = torch.LongTensor(len(batch), max_input_len)\n",
        "    mel_padded = torch.FloatTensor(len(batch), 80, max_target_len)\n",
        "    gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "    output_lengths = torch.LongTensor(len(batch))\n",
        "\n",
        "    text_padded.zero_()\n",
        "    mel_padded.zero_()\n",
        "    gate_padded.zero_()\n",
        "\n",
        "    for i in range(len(ids_sorted_decreasing)):\n",
        "        _, _, _, text = batch[ids_sorted_decreasing[i]]\n",
        "        mel = mel_list[ids_sorted_decreasing[i]]\n",
        "        mel = self.dynamic_range_compression(mel)\n",
        "        mel_padded[i, :, :mel.size(1)] = mel\n",
        "        gate_padded[i, mel.size(1)-1:] = 1\n",
        "        output_lengths[i] = mel.size(1)\n",
        "        text = text_normalize(text)\n",
        "        text = [char2idx[char] for char in text]\n",
        "        text_norm = torch.IntTensor(text)\n",
        "        text_padded[i, :len(text)] = text_norm\n",
        "\n",
        "    return text_padded, input_lengths, mel_padded, gate_padded, output_lengths\n",
        "\n",
        "  def dynamic_range_compression(self, x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "collate_fn = Collate()\n",
        "train_loader = DataLoader(train_dataset, batch_size=hparams.batch_size, shuffle=True, drop_last=False, collate_fn=collate_fn)\n",
        "\n",
        "print('Data 전처리 완료')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data 전처리 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z9sMYEOrGrE"
      },
      "source": [
        "def get_mask_from_lengths(lengths):\n",
        "    max_len = torch.max(lengths).item()\n",
        "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
        "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    x = x.contiguous() #연산과정에서 메모리에 올려진 순서에 따라 발생할 수 있는 비효율성 혹은 최악의 경우 에러를 방지하기 위함\n",
        "                        #예를들어 transpose의 경우 메모리에 올려진 순서가 연속성이 깨지게되는데 메모리 접근 성능이 비효율적으로됨\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(non_blocking=True)\n",
        "    return torch.autograd.Variable(x)\n",
        "\n",
        "\n",
        "class LinearNorm(torch.nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
        "      super(LinearNorm, self).__init__()\n",
        "      self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(\n",
        "          self.linear_layer.weight,\n",
        "          gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.linear_layer(x)\n",
        "\n",
        "\n",
        "class ConvNorm(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
        "                padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
        "      super(ConvNorm, self).__init__()\n",
        "      if padding is None:\n",
        "          assert(kernel_size % 2 == 1)\n",
        "          padding = int(dilation * (kernel_size - 1) / 2)\n",
        "\n",
        "      self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
        "                                  kernel_size=kernel_size, stride=stride,\n",
        "                                  padding=padding, dilation=dilation,\n",
        "                                  bias=bias)\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(\n",
        "          self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
        "\n",
        "  def forward(self, signal):\n",
        "      conv_signal = self.conv(signal)\n",
        "      return conv_signal"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqVD-yf9rKVW"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\"Encoder module:\n",
        "      - Three 1-d convolution banks\n",
        "      - Bidirectional LSTM\n",
        "  \"\"\"\n",
        "  def __init__(self, hparams):\n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      convolutions = []\n",
        "      for _ in range(hparams.encoder_n_convolutions):\n",
        "          conv_layer = nn.Sequential(\n",
        "              ConvNorm(hparams.encoder_embedding_dim,\n",
        "                        hparams.encoder_embedding_dim,\n",
        "                        kernel_size=hparams.encoder_kernel_size, stride=1,\n",
        "                        padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
        "                        dilation=1, w_init_gain='relu'),\n",
        "              nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
        "          convolutions.append(conv_layer)\n",
        "      self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "      self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                          int(hparams.encoder_embedding_dim / 2), 1,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "\n",
        "  def forward(self, x, input_lengths):\n",
        "      for conv in self.convolutions:\n",
        "          x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "      x = x.transpose(1, 2)\n",
        "\n",
        "      # pytorch tensor are not reversible, hence the conversion\n",
        "      input_lengths = input_lengths.cpu().numpy()\n",
        "      x = nn.utils.rnn.pack_padded_sequence(\n",
        "          x, input_lengths, batch_first=True) #병렬 처리를 위해 padded된 sequence들을 packing하는 과정, 일종의 길이에 따른 sorting 과정 더 빠른 연산 가능\n",
        "\n",
        "      self.lstm.flatten_parameters()\n",
        "      outputs, _ = self.lstm(x)\n",
        "\n",
        "      outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "          outputs, batch_first=True)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "  def inference(self, x):\n",
        "      for conv in self.convolutions:\n",
        "          x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
        "\n",
        "      x = x.transpose(1, 2)\n",
        "\n",
        "      self.lstm.flatten_parameters()\n",
        "      outputs, _ = self.lstm(x)\n",
        "\n",
        "      return outputs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dD0kI-qrOCl"
      },
      "source": [
        "class LocationLayer(nn.Module):\n",
        "  def __init__(self, attention_n_filters, attention_kernel_size,\n",
        "                attention_dim):\n",
        "      super(LocationLayer, self).__init__()\n",
        "      padding = int((attention_kernel_size - 1) / 2)\n",
        "      self.location_conv = ConvNorm(2, attention_n_filters,\n",
        "                                    kernel_size=attention_kernel_size,\n",
        "                                    padding=padding, bias=False, stride=1,\n",
        "                                    dilation=1)\n",
        "      self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
        "                                        bias=False, w_init_gain='tanh')\n",
        "\n",
        "  def forward(self, attention_weights_cat):\n",
        "      processed_attention = self.location_conv(attention_weights_cat)\n",
        "      processed_attention = processed_attention.transpose(1, 2)\n",
        "      processed_attention = self.location_dense(processed_attention)\n",
        "      return processed_attention\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "                attention_location_n_filters, attention_location_kernel_size):\n",
        "      super(Attention, self).__init__()\n",
        "      self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "                                    bias=False, w_init_gain='tanh')\n",
        "      self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "                                      w_init_gain='tanh')\n",
        "      self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "      self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "                                          attention_location_kernel_size,\n",
        "                                          attention_dim)\n",
        "      self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "  def get_alignment_energies(self, query, processed_memory,\n",
        "                              attention_weights_cat):\n",
        "      \"\"\"\n",
        "      PARAMS\n",
        "      ------\n",
        "      query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
        "      processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
        "      attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      alignment (batch, max_time)\n",
        "      \"\"\"\n",
        "\n",
        "      processed_query = self.query_layer(query.unsqueeze(1))\n",
        "      processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "      energies = self.v(torch.tanh(\n",
        "          processed_query + processed_attention_weights + processed_memory))\n",
        "\n",
        "      energies = energies.squeeze(-1)\n",
        "      return energies\n",
        "\n",
        "  def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "              attention_weights_cat, mask):\n",
        "      \"\"\"\n",
        "      PARAMS\n",
        "      ------\n",
        "      attention_hidden_state: attention rnn last output\n",
        "      memory: encoder outputs\n",
        "      processed_memory: processed encoder outputs\n",
        "      attention_weights_cat: previous and cummulative attention weights\n",
        "      mask: binary mask for padded data\n",
        "      \"\"\"\n",
        "      alignment = self.get_alignment_energies(\n",
        "          attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "      if mask is not None:\n",
        "          alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "      attention_weights = F.softmax(alignment, dim=1)\n",
        "      attention_context = torch.bmm(attention_weights.unsqueeze(1), memory) # bmm: batch matrix multiplication\n",
        "      attention_context = attention_context.squeeze(1)\n",
        "\n",
        "      return attention_context, attention_weights\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEA_-jZHrQdr"
      },
      "source": [
        "\n",
        "class Prenet(nn.Module):\n",
        "  def __init__(self, in_dim, sizes):\n",
        "      super(Prenet, self).__init__()\n",
        "      in_sizes = [in_dim] + sizes[:-1]\n",
        "      self.layers = nn.ModuleList(\n",
        "          [LinearNorm(in_size, out_size, bias=False)\n",
        "            for (in_size, out_size) in zip(in_sizes, sizes)])\n",
        "\n",
        "  def forward(self, x):\n",
        "      for linear in self.layers:\n",
        "          x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
        "      return x\n",
        "\n",
        "\n",
        "class Postnet(nn.Module):\n",
        "  \"\"\"Postnet\n",
        "      - Five 1-d convolution with 512 channels and kernel size 5\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hparams):\n",
        "      super(Postnet, self).__init__()\n",
        "      self.convolutions = nn.ModuleList()\n",
        "\n",
        "      self.convolutions.append(\n",
        "          nn.Sequential(\n",
        "              ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n",
        "                        kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                        padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                        dilation=1, w_init_gain='tanh'),\n",
        "              nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "      )\n",
        "\n",
        "      for i in range(1, hparams.postnet_n_convolutions - 1):\n",
        "          self.convolutions.append(\n",
        "              nn.Sequential(\n",
        "                  ConvNorm(hparams.postnet_embedding_dim,\n",
        "                            hparams.postnet_embedding_dim,\n",
        "                            kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                            padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                            dilation=1, w_init_gain='tanh'),\n",
        "                  nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "          )\n",
        "\n",
        "      self.convolutions.append(\n",
        "          nn.Sequential(\n",
        "              ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n",
        "                        kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                        padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                        dilation=1, w_init_gain='linear'),\n",
        "              nn.BatchNorm1d(hparams.n_mel_channels))\n",
        "          )\n",
        "\n",
        "  def forward(self, x):\n",
        "      for i in range(len(self.convolutions) - 1):\n",
        "          x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
        "      x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.n_mel_channels = hparams.n_mel_channels\n",
        "      self.n_frames_per_step = hparams.n_frames_per_step\n",
        "      self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
        "      self.attention_rnn_dim = hparams.attention_rnn_dim\n",
        "      self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
        "      self.prenet_dim = hparams.prenet_dim\n",
        "      self.max_decoder_steps = hparams.max_decoder_steps\n",
        "      self.gate_threshold = hparams.gate_threshold\n",
        "      self.p_attention_dropout = hparams.p_attention_dropout\n",
        "      self.p_decoder_dropout = hparams.p_decoder_dropout\n",
        "\n",
        "      self.prenet = Prenet(\n",
        "          hparams.n_mel_channels * hparams.n_frames_per_step,\n",
        "          [hparams.prenet_dim, hparams.prenet_dim])\n",
        "\n",
        "      self.attention_rnn = nn.LSTMCell(\n",
        "          hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
        "          hparams.attention_rnn_dim)\n",
        "\n",
        "      self.attention_layer = Attention(\n",
        "          hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
        "          hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "          hparams.attention_location_kernel_size)\n",
        "\n",
        "      self.decoder_rnn = nn.LSTMCell(\n",
        "          hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
        "          hparams.decoder_rnn_dim, 1)\n",
        "\n",
        "      self.linear_projection = LinearNorm(\n",
        "          hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
        "          hparams.n_mel_channels * hparams.n_frames_per_step)\n",
        "\n",
        "      self.gate_layer = LinearNorm(\n",
        "          hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
        "          bias=True, w_init_gain='sigmoid')\n",
        "\n",
        "  def get_go_frame(self, memory):\n",
        "      \"\"\" Gets all zeros frames to use as first decoder input\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: decoder outputs\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      decoder_input: all zeros frames\n",
        "      \"\"\"\n",
        "      B = memory.size(0)\n",
        "      decoder_input = Variable(memory.data.new(\n",
        "          B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
        "      return decoder_input\n",
        "\n",
        "  def initialize_decoder_states(self, memory, mask):\n",
        "      \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
        "      weights, attention cumulative weights, attention context, stores memory\n",
        "      and stores processed memory\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: Encoder outputs\n",
        "      mask: Mask for padded data if training, expects None for inference\n",
        "      \"\"\"\n",
        "      B = memory.size(0)\n",
        "      MAX_TIME = memory.size(1)\n",
        "\n",
        "      self.attention_hidden = Variable(memory.data.new(\n",
        "          B, self.attention_rnn_dim).zero_())\n",
        "      self.attention_cell = Variable(memory.data.new(\n",
        "          B, self.attention_rnn_dim).zero_())\n",
        "\n",
        "      self.decoder_hidden = Variable(memory.data.new(\n",
        "          B, self.decoder_rnn_dim).zero_())\n",
        "      self.decoder_cell = Variable(memory.data.new(\n",
        "          B, self.decoder_rnn_dim).zero_())\n",
        "\n",
        "      self.attention_weights = Variable(memory.data.new(\n",
        "          B, MAX_TIME).zero_())\n",
        "      self.attention_weights_cum = Variable(memory.data.new(\n",
        "          B, MAX_TIME).zero_())\n",
        "      self.attention_context = Variable(memory.data.new(\n",
        "          B, self.encoder_embedding_dim).zero_())\n",
        "\n",
        "      self.memory = memory\n",
        "      self.processed_memory = self.attention_layer.memory_layer(memory)\n",
        "      self.mask = mask\n",
        "\n",
        "  def parse_decoder_inputs(self, decoder_inputs):\n",
        "      \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
        "      PARAMS\n",
        "      ------\n",
        "      decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      inputs: processed decoder inputs\n",
        "\n",
        "      \"\"\"\n",
        "      # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "      decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "      decoder_inputs = decoder_inputs.view(\n",
        "          decoder_inputs.size(0),\n",
        "          int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
        "      # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "      decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "      return decoder_inputs\n",
        "\n",
        "  def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
        "      \"\"\" Prepares decoder outputs for output\n",
        "      PARAMS\n",
        "      ------\n",
        "      mel_outputs:\n",
        "      gate_outputs: gate output energies\n",
        "      alignments:\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_outputs:\n",
        "      gate_outpust: gate output energies\n",
        "      alignments:\n",
        "      \"\"\"\n",
        "      # (T_out, B) -> (B, T_out)\n",
        "      alignments = torch.stack(alignments).transpose(0, 1)\n",
        "      # (T_out, B) -> (B, T_out)\n",
        "      gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
        "      gate_outputs = gate_outputs.contiguous()\n",
        "      # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "      mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "      # decouple frames per step\n",
        "      mel_outputs = mel_outputs.view(\n",
        "          mel_outputs.size(0), -1, self.n_mel_channels)\n",
        "      # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "      mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "      return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "  def decode(self, decoder_input):\n",
        "      \"\"\" Decoder step using stored states, attention and memory\n",
        "      PARAMS\n",
        "      ------\n",
        "      decoder_input: previous mel output\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_output:\n",
        "      gate_output: gate output energies\n",
        "      attention_weights:\n",
        "      \"\"\"\n",
        "      cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
        "      self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
        "          cell_input, (self.attention_hidden, self.attention_cell))\n",
        "      self.attention_hidden = F.dropout(\n",
        "          self.attention_hidden, self.p_attention_dropout, self.training)\n",
        "\n",
        "      attention_weights_cat = torch.cat(\n",
        "          (self.attention_weights.unsqueeze(1),\n",
        "            self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "      self.attention_context, self.attention_weights = self.attention_layer(\n",
        "          self.attention_hidden, self.memory, self.processed_memory,\n",
        "          attention_weights_cat, self.mask) \n",
        "\n",
        "      self.attention_weights_cum += self.attention_weights\n",
        "      decoder_input = torch.cat(\n",
        "          (self.attention_hidden, self.attention_context), -1)\n",
        "      self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
        "          decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
        "      self.decoder_hidden = F.dropout(\n",
        "          self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
        "\n",
        "      decoder_hidden_attention_context = torch.cat(\n",
        "          (self.decoder_hidden, self.attention_context), dim=1)\n",
        "      decoder_output = self.linear_projection(\n",
        "          decoder_hidden_attention_context)\n",
        "\n",
        "      gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
        "      return decoder_output, gate_prediction, self.attention_weights\n",
        "\n",
        "  def forward(self, memory, decoder_inputs, memory_lengths):\n",
        "      \"\"\" Decoder forward pass for training\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: Encoder outputs\n",
        "      decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
        "      memory_lengths: Encoder output lengths for attention masking.\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_outputs: mel outputs from the decoder\n",
        "      gate_outputs: gate outputs from the decoder\n",
        "      alignments: sequence of attention weights from the decoder\n",
        "      \"\"\"\n",
        "\n",
        "      decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
        "      decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
        "      decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
        "      decoder_inputs = self.prenet(decoder_inputs)\n",
        "\n",
        "      self.initialize_decoder_states(\n",
        "          memory, mask=~get_mask_from_lengths(memory_lengths))\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = [], [], []\n",
        "      while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "          decoder_input = decoder_inputs[len(mel_outputs)]\n",
        "          mel_output, gate_output, attention_weights = self.decode(\n",
        "              decoder_input)\n",
        "          mel_outputs += [mel_output.squeeze(1)]\n",
        "          gate_outputs += [gate_output.squeeze(1)]\n",
        "          alignments += [attention_weights]\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "          mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "      return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "  def inference(self, memory):\n",
        "      \"\"\" Decoder inference\n",
        "      PARAMS\n",
        "      ------\n",
        "      memory: Encoder outputs\n",
        "\n",
        "      RETURNS\n",
        "      -------\n",
        "      mel_outputs: mel outputs from the decoder\n",
        "      gate_outputs: gate outputs from the decoder\n",
        "      alignments: sequence of attention weights from the decoder\n",
        "      \"\"\"\n",
        "      decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "      self.initialize_decoder_states(memory, mask=None)\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = [], [], []\n",
        "      while True: #free learning 모드\n",
        "          decoder_input = self.prenet(decoder_input)\n",
        "          mel_output, gate_output, alignment = self.decode(decoder_input)\n",
        "\n",
        "          mel_outputs += [mel_output.squeeze(1)]\n",
        "          gate_outputs += [gate_output]\n",
        "          alignments += [alignment]\n",
        "\n",
        "          if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
        "              break\n",
        "          elif len(mel_outputs) == self.max_decoder_steps:\n",
        "              print(\"Warning! Reached max decoder steps\")\n",
        "              break\n",
        "\n",
        "          decoder_input = mel_output\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "          mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "      return mel_outputs, gate_outputs, alignments\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2oSqMCTrSSS"
      },
      "source": [
        "class Tacotron2(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "      super(Tacotron2, self).__init__()\n",
        "      self.mask_padding = hparams.mask_padding\n",
        "      self.n_mel_channels = hparams.n_mel_channels\n",
        "      self.n_frames_per_step = hparams.n_frames_per_step\n",
        "      self.embedding = nn.Embedding(\n",
        "          len(vocab), hparams.symbols_embedding_dim)\n",
        "      std = sqrt(2.0 / (len(vocab) + hparams.symbols_embedding_dim))\n",
        "      val = sqrt(3.0) * std  # uniform bounds for std\n",
        "      self.embedding.weight.data.uniform_(-val, val)\n",
        "      self.encoder = Encoder(hparams)\n",
        "      self.decoder = Decoder(hparams)\n",
        "      self.postnet = Postnet(hparams)\n",
        "\n",
        "  def parse_batch(self, batch):\n",
        "      text_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "          output_lengths = batch\n",
        "      text_padded = to_gpu(text_padded).long()\n",
        "      input_lengths = to_gpu(input_lengths).long()\n",
        "      max_len = torch.max(input_lengths.data).item()\n",
        "      mel_padded = to_gpu(mel_padded).float()\n",
        "      gate_padded = to_gpu(gate_padded).float()\n",
        "      output_lengths = to_gpu(output_lengths).long()\n",
        "\n",
        "      return (\n",
        "          (text_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
        "          (mel_padded, gate_padded))\n",
        "\n",
        "  def parse_output(self, outputs, output_lengths=None):\n",
        "      if self.mask_padding and output_lengths is not None:\n",
        "          mask = ~get_mask_from_lengths(output_lengths)\n",
        "          mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
        "          mask = mask.permute(1, 0, 2)\n",
        "\n",
        "          outputs[0].data.masked_fill_(mask, 0.0)\n",
        "          outputs[1].data.masked_fill_(mask, 0.0)\n",
        "          outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "      return outputs\n",
        "\n",
        "  def forward(self, inputs):\n",
        "      text_inputs, text_lengths, mels, max_len, output_lengths = inputs\n",
        "      text_lengths, output_lengths = text_lengths.data, output_lengths.data\n",
        "\n",
        "      embedded_inputs = self.embedding(text_inputs).transpose(1, 2) #LUT\n",
        "\n",
        "      encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
        "\n",
        "      mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "          encoder_outputs, mels, memory_lengths=text_lengths)\n",
        "\n",
        "      mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "      mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "      return self.parse_output(\n",
        "          [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "          output_lengths)\n",
        "\n",
        "  def inference(self, inputs):\n",
        "      embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
        "      encoder_outputs = self.encoder.inference(embedded_inputs)\n",
        "      mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
        "          encoder_outputs)\n",
        "\n",
        "      mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "      mel_outputs_postnet = mel_outputs + mel_ouftputs_postnet\n",
        "\n",
        "      outputs = self.parse_output(\n",
        "          [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
        "\n",
        "      return outputs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqlrQsMArTQ_"
      },
      "source": [
        "model = Tacotron2(hparams).to(device)\n",
        "learning_rate = hparams.learning_rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                              weight_decay=hparams.weight_decay)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2FAV9orUVm"
      },
      "source": [
        "class Tacotron2Loss(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Tacotron2Loss, self).__init__()\n",
        "\n",
        "  def forward(self, model_output, targets):\n",
        "      mel_target, gate_target = targets[0], targets[1]\n",
        "      mel_target.requires_grad = False\n",
        "      gate_target.requires_grad = False\n",
        "      gate_target = gate_target.view(-1, 1)\n",
        "\n",
        "      mel_out, mel_out_postnet, gate_out, _ = model_output\n",
        "      gate_out = gate_out.view(-1, 1)\n",
        "      mel_loss = nn.MSELoss()(mel_out, mel_target) + nn.MSELoss()(mel_out_postnet, mel_target)\n",
        "      gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target) #binary cross entropy (BCE보다 안정적)\n",
        "      return mel_loss + gate_loss\n",
        "\n",
        "criterion = Tacotron2Loss()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkgCN3PXrXw4",
        "outputId": "65a4df33-2479-4484-9d91-65ffd9fb899c"
      },
      "source": [
        "iteration = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "  print(\"Epoch: {}\".format(epoch))\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    start = time.perf_counter()\n",
        "    model.zero_grad()\n",
        "    x, y = model.parse_batch(batch)\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    reduced_loss = loss.item()  #[1.0] -> 1.0\n",
        "    loss.backward()\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "        model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    duration = time.perf_counter() - start\n",
        "    print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n",
        "        iteration, reduced_loss, grad_norm, duration))\n",
        "    \n",
        "    iteration += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train loss 0 41.262115 Grad Norm 10.090382 3.83s/it\n",
            "Train loss 1 34.182240 Grad Norm 14.962674 4.77s/it\n",
            "Train loss 2 27.197054 Grad Norm 12.481231 4.74s/it\n",
            "Train loss 3 19.453251 Grad Norm 6.599640 4.71s/it\n",
            "Train loss 4 15.709000 Grad Norm 7.252130 5.00s/it\n",
            "Train loss 5 17.465862 Grad Norm 11.515422 4.25s/it\n",
            "Train loss 6 18.125471 Grad Norm 10.454226 4.72s/it\n",
            "Train loss 7 20.764355 Grad Norm 6.371534 3.78s/it\n",
            "Train loss 8 14.647317 Grad Norm 4.589945 4.52s/it\n",
            "Train loss 9 16.107910 Grad Norm 4.782270 4.49s/it\n",
            "Train loss 10 12.655185 Grad Norm 5.330668 5.04s/it\n",
            "Train loss 11 12.222188 Grad Norm 4.347077 5.17s/it\n",
            "Train loss 12 14.551733 Grad Norm 4.798047 4.78s/it\n",
            "Train loss 13 12.332122 Grad Norm 4.409626 4.59s/it\n",
            "Train loss 14 13.164663 Grad Norm 4.917517 4.79s/it\n",
            "Train loss 15 9.690498 Grad Norm 3.671657 4.92s/it\n",
            "Train loss 16 9.500235 Grad Norm 5.952421 4.77s/it\n",
            "Train loss 17 8.708433 Grad Norm 2.363390 4.45s/it\n",
            "Train loss 18 8.827098 Grad Norm 5.153114 3.97s/it\n",
            "Train loss 19 10.562172 Grad Norm 7.887605 4.32s/it\n",
            "Train loss 20 9.522230 Grad Norm 3.689388 5.10s/it\n",
            "Train loss 21 9.440473 Grad Norm 5.592544 4.52s/it\n",
            "Train loss 22 10.464619 Grad Norm 4.238861 4.45s/it\n",
            "Train loss 23 9.597775 Grad Norm 3.865539 4.44s/it\n",
            "Train loss 24 9.893117 Grad Norm 6.818898 4.95s/it\n",
            "Train loss 25 8.375016 Grad Norm 3.957093 5.15s/it\n",
            "Train loss 26 9.315968 Grad Norm 4.600821 4.75s/it\n",
            "Train loss 27 8.097276 Grad Norm 4.420207 4.68s/it\n",
            "Train loss 28 8.708899 Grad Norm 4.311252 5.07s/it\n",
            "Train loss 29 10.247043 Grad Norm 4.254425 2.67s/it\n",
            "Train loss 30 8.986038 Grad Norm 2.286670 4.94s/it\n",
            "Train loss 31 9.256996 Grad Norm 4.389005 3.75s/it\n",
            "Train loss 32 11.371487 Grad Norm 5.451972 3.43s/it\n",
            "Train loss 33 7.674623 Grad Norm 3.218618 3.10s/it\n",
            "Train loss 34 8.483966 Grad Norm 3.614614 4.64s/it\n",
            "Train loss 35 8.514166 Grad Norm 4.321193 3.27s/it\n",
            "Train loss 36 8.628922 Grad Norm 2.733677 3.95s/it\n",
            "Train loss 37 7.526421 Grad Norm 6.328945 4.44s/it\n",
            "Train loss 38 7.178987 Grad Norm 5.265192 5.00s/it\n",
            "Train loss 39 7.576098 Grad Norm 3.817878 5.17s/it\n",
            "Train loss 40 8.834139 Grad Norm 7.285770 4.68s/it\n",
            "Train loss 41 7.567783 Grad Norm 3.253118 4.56s/it\n",
            "Train loss 42 7.896927 Grad Norm 2.968430 4.80s/it\n",
            "Train loss 43 8.420035 Grad Norm 3.335788 4.31s/it\n",
            "Train loss 44 7.091918 Grad Norm 4.008868 4.77s/it\n",
            "Train loss 45 8.844929 Grad Norm 4.397456 3.47s/it\n",
            "Train loss 46 7.347423 Grad Norm 3.708079 4.53s/it\n",
            "Train loss 47 6.423262 Grad Norm 2.915962 5.08s/it\n",
            "Train loss 48 8.846050 Grad Norm 4.580168 4.26s/it\n",
            "Train loss 49 8.447367 Grad Norm 4.285178 5.12s/it\n",
            "Train loss 50 8.182321 Grad Norm 3.662213 4.90s/it\n",
            "Train loss 51 7.520107 Grad Norm 3.936358 4.75s/it\n",
            "Train loss 52 7.962540 Grad Norm 3.397669 4.94s/it\n",
            "Train loss 53 7.381958 Grad Norm 3.023968 4.16s/it\n",
            "Train loss 54 6.841922 Grad Norm 3.672261 5.16s/it\n",
            "Train loss 55 7.110114 Grad Norm 2.348167 3.94s/it\n",
            "Train loss 56 7.729623 Grad Norm 4.866026 5.03s/it\n",
            "Train loss 57 6.401816 Grad Norm 4.956496 5.03s/it\n",
            "Train loss 58 7.010995 Grad Norm 2.764371 3.96s/it\n",
            "Train loss 59 8.267407 Grad Norm 4.022459 3.72s/it\n",
            "Train loss 60 7.298958 Grad Norm 2.009923 3.87s/it\n",
            "Train loss 61 8.313068 Grad Norm 2.394534 4.24s/it\n",
            "Train loss 62 6.804444 Grad Norm 2.025877 4.33s/it\n",
            "Train loss 63 6.838568 Grad Norm 2.046032 4.63s/it\n",
            "Train loss 64 7.897681 Grad Norm 3.893477 4.99s/it\n",
            "Train loss 65 6.904943 Grad Norm 2.841182 4.83s/it\n",
            "Train loss 66 7.553233 Grad Norm 2.960613 4.61s/it\n",
            "Train loss 67 5.658602 Grad Norm 2.103318 3.18s/it\n",
            "Train loss 68 7.565187 Grad Norm 4.070754 4.77s/it\n",
            "Train loss 69 5.802880 Grad Norm 2.782581 2.46s/it\n",
            "Train loss 70 6.352041 Grad Norm 2.857558 5.05s/it\n",
            "Train loss 71 6.740843 Grad Norm 2.511069 4.98s/it\n",
            "Train loss 72 6.548277 Grad Norm 2.052731 4.55s/it\n",
            "Train loss 73 6.099101 Grad Norm 2.245993 4.38s/it\n",
            "Train loss 74 7.400572 Grad Norm 2.992380 5.11s/it\n",
            "Train loss 75 5.420164 Grad Norm 2.531150 4.09s/it\n",
            "Train loss 76 6.737383 Grad Norm 2.138208 5.01s/it\n",
            "Train loss 77 5.662821 Grad Norm 2.123132 5.00s/it\n",
            "Train loss 78 6.869680 Grad Norm 4.724073 4.36s/it\n",
            "Train loss 79 5.754704 Grad Norm 2.207766 5.11s/it\n",
            "Train loss 80 8.554813 Grad Norm 5.217505 4.19s/it\n",
            "Train loss 81 6.724358 Grad Norm 2.775694 4.69s/it\n",
            "Train loss 82 7.659463 Grad Norm 4.570365 4.15s/it\n",
            "Train loss 83 6.844999 Grad Norm 2.836013 4.99s/it\n",
            "Train loss 84 5.678704 Grad Norm 2.174677 4.08s/it\n",
            "Train loss 85 6.419022 Grad Norm 3.795107 4.67s/it\n",
            "Train loss 86 6.918944 Grad Norm 2.697589 4.70s/it\n",
            "Train loss 87 6.500139 Grad Norm 3.263889 4.39s/it\n",
            "Train loss 88 6.431126 Grad Norm 2.905040 5.06s/it\n",
            "Train loss 89 4.659763 Grad Norm 2.857367 4.63s/it\n",
            "Train loss 90 4.991481 Grad Norm 3.897887 4.87s/it\n",
            "Train loss 91 7.142984 Grad Norm 14.864363 3.62s/it\n",
            "Train loss 92 6.032117 Grad Norm 9.549790 4.05s/it\n",
            "Train loss 93 6.953918 Grad Norm 10.027235 3.80s/it\n",
            "Train loss 94 6.648695 Grad Norm 5.469902 4.75s/it\n",
            "Train loss 95 7.482048 Grad Norm 4.257909 4.86s/it\n",
            "Train loss 96 6.440935 Grad Norm 4.913484 4.51s/it\n",
            "Train loss 97 6.195045 Grad Norm 2.544227 4.15s/it\n",
            "Train loss 98 6.814608 Grad Norm 4.018017 4.03s/it\n",
            "Train loss 99 7.197135 Grad Norm 3.710276 4.55s/it\n",
            "Train loss 100 4.487046 Grad Norm 4.716551 4.35s/it\n",
            "Train loss 101 6.608263 Grad Norm 5.101899 3.08s/it\n",
            "Train loss 102 7.019222 Grad Norm 2.747504 3.44s/it\n",
            "Train loss 103 6.060753 Grad Norm 2.741175 4.55s/it\n",
            "Train loss 104 6.762375 Grad Norm 4.712271 1.97s/it\n",
            "Train loss 105 7.200773 Grad Norm 4.080107 5.16s/it\n",
            "Train loss 106 6.957313 Grad Norm 3.082233 4.03s/it\n",
            "Train loss 107 4.875492 Grad Norm 4.803089 5.07s/it\n",
            "Train loss 108 6.216036 Grad Norm 5.007467 4.37s/it\n",
            "Train loss 109 6.109791 Grad Norm 3.799827 4.25s/it\n",
            "Train loss 110 5.840665 Grad Norm 4.392750 3.08s/it\n",
            "Train loss 111 6.714170 Grad Norm 5.586391 3.59s/it\n",
            "Train loss 112 6.066151 Grad Norm 4.673646 4.45s/it\n",
            "Train loss 113 5.768941 Grad Norm 3.908662 4.01s/it\n",
            "Train loss 114 7.399282 Grad Norm 9.454423 4.45s/it\n",
            "Train loss 115 6.396428 Grad Norm 3.828174 5.01s/it\n",
            "Train loss 116 6.607869 Grad Norm 4.274839 4.79s/it\n",
            "Train loss 117 6.212138 Grad Norm 4.304976 3.70s/it\n",
            "Train loss 118 6.351266 Grad Norm 3.907529 4.30s/it\n",
            "Train loss 119 6.350519 Grad Norm 4.188327 4.45s/it\n",
            "Train loss 120 4.758482 Grad Norm 4.140996 4.57s/it\n",
            "Train loss 121 5.756173 Grad Norm 3.181073 4.39s/it\n",
            "Train loss 122 6.451380 Grad Norm 4.011583 4.12s/it\n",
            "Train loss 123 5.601862 Grad Norm 2.979934 4.82s/it\n",
            "Train loss 124 5.527065 Grad Norm 2.439988 4.71s/it\n",
            "Train loss 125 5.942324 Grad Norm 2.873122 4.28s/it\n",
            "Train loss 126 6.517271 Grad Norm 3.370918 4.75s/it\n",
            "Train loss 127 7.000645 Grad Norm 8.054131 4.41s/it\n",
            "Train loss 128 5.224410 Grad Norm 3.123633 5.13s/it\n",
            "Train loss 129 5.725814 Grad Norm 3.559458 5.08s/it\n",
            "Train loss 130 5.542906 Grad Norm 3.829340 4.70s/it\n",
            "Train loss 131 5.265843 Grad Norm 3.372193 4.89s/it\n",
            "Train loss 132 5.076956 Grad Norm 3.759429 4.43s/it\n",
            "Train loss 133 4.393996 Grad Norm 2.245214 3.81s/it\n",
            "Train loss 134 5.189615 Grad Norm 3.278640 5.01s/it\n",
            "Train loss 135 6.328168 Grad Norm 5.109965 3.19s/it\n",
            "Train loss 136 5.313762 Grad Norm 2.706888 5.00s/it\n",
            "Train loss 137 4.496636 Grad Norm 2.385298 5.18s/it\n",
            "Train loss 138 5.841021 Grad Norm 2.799824 4.76s/it\n",
            "Train loss 139 5.456624 Grad Norm 5.356767 3.33s/it\n",
            "Train loss 140 5.611279 Grad Norm 3.207545 4.70s/it\n",
            "Train loss 141 5.018810 Grad Norm 2.191438 4.74s/it\n",
            "Train loss 142 5.413536 Grad Norm 1.998105 5.08s/it\n",
            "Train loss 143 6.097316 Grad Norm 3.570772 3.31s/it\n",
            "Train loss 144 6.358592 Grad Norm 3.419022 3.56s/it\n",
            "Train loss 145 6.207637 Grad Norm 3.744643 4.79s/it\n",
            "Train loss 146 6.706918 Grad Norm 3.368541 5.03s/it\n",
            "Train loss 147 4.837484 Grad Norm 2.572830 3.47s/it\n",
            "Train loss 148 5.540516 Grad Norm 4.302010 5.11s/it\n",
            "Train loss 149 6.083323 Grad Norm 3.088784 4.83s/it\n",
            "Train loss 150 5.650348 Grad Norm 4.311535 4.98s/it\n",
            "Train loss 151 4.792952 Grad Norm 4.189559 4.51s/it\n",
            "Train loss 152 6.023872 Grad Norm 3.428752 4.97s/it\n",
            "Train loss 153 5.000124 Grad Norm 2.304598 4.39s/it\n",
            "Train loss 154 6.153562 Grad Norm 4.569534 3.83s/it\n",
            "Train loss 155 5.246791 Grad Norm 4.023058 4.45s/it\n",
            "Train loss 156 6.157405 Grad Norm 5.127945 4.88s/it\n",
            "Train loss 157 5.608876 Grad Norm 2.837942 4.62s/it\n",
            "Train loss 158 4.938921 Grad Norm 2.176397 4.94s/it\n",
            "Train loss 159 4.196095 Grad Norm 2.771488 4.75s/it\n",
            "Train loss 160 6.602045 Grad Norm 7.858526 3.60s/it\n",
            "Train loss 161 6.240986 Grad Norm 3.960121 3.03s/it\n",
            "Train loss 162 5.185930 Grad Norm 3.149286 4.62s/it\n",
            "Train loss 163 5.361223 Grad Norm 4.650041 4.29s/it\n",
            "Train loss 164 5.927351 Grad Norm 3.463467 4.30s/it\n",
            "Train loss 165 5.041203 Grad Norm 2.017680 4.92s/it\n",
            "Train loss 166 5.578916 Grad Norm 2.314173 4.35s/it\n",
            "Train loss 167 5.226278 Grad Norm 2.859014 4.45s/it\n",
            "Train loss 168 6.409525 Grad Norm 3.452661 4.66s/it\n",
            "Train loss 169 5.251527 Grad Norm 2.646360 4.19s/it\n",
            "Train loss 170 5.539336 Grad Norm 5.694476 3.68s/it\n",
            "Train loss 171 5.329993 Grad Norm 4.304885 4.83s/it\n",
            "Train loss 172 4.903492 Grad Norm 2.093904 4.20s/it\n",
            "Train loss 173 5.114396 Grad Norm 3.244525 5.16s/it\n",
            "Train loss 174 5.409675 Grad Norm 2.640399 3.92s/it\n",
            "Train loss 175 4.594530 Grad Norm 2.297738 4.78s/it\n",
            "Train loss 176 4.953056 Grad Norm 1.571577 4.87s/it\n",
            "Train loss 177 5.643330 Grad Norm 2.068932 4.87s/it\n",
            "Train loss 178 5.925136 Grad Norm 2.972589 3.99s/it\n",
            "Train loss 179 4.404455 Grad Norm 1.914685 4.47s/it\n",
            "Train loss 180 5.982758 Grad Norm 2.276776 4.64s/it\n",
            "Train loss 181 3.230283 Grad Norm 2.566271 5.02s/it\n",
            "Train loss 182 5.159902 Grad Norm 5.022879 3.17s/it\n",
            "Train loss 183 6.065636 Grad Norm 5.563920 4.69s/it\n",
            "Train loss 184 6.044697 Grad Norm 2.717894 3.86s/it\n",
            "Train loss 185 5.213353 Grad Norm 5.714183 4.71s/it\n",
            "Train loss 186 5.719413 Grad Norm 2.717958 3.33s/it\n",
            "Train loss 187 4.711339 Grad Norm 1.991951 4.57s/it\n",
            "Train loss 188 5.521214 Grad Norm 6.434905 4.30s/it\n",
            "Train loss 189 4.860618 Grad Norm 4.847369 5.04s/it\n",
            "Train loss 190 6.968053 Grad Norm 5.341397 4.89s/it\n",
            "Train loss 191 5.089157 Grad Norm 2.608817 3.82s/it\n",
            "Train loss 192 5.305738 Grad Norm 4.129503 3.33s/it\n",
            "Train loss 193 6.361984 Grad Norm 6.322059 4.46s/it\n",
            "Train loss 194 5.414300 Grad Norm 3.570621 3.59s/it\n",
            "Train loss 195 5.850561 Grad Norm 5.331041 3.08s/it\n",
            "Train loss 196 4.010213 Grad Norm 4.126957 4.72s/it\n",
            "Train loss 197 5.010518 Grad Norm 2.552585 3.96s/it\n",
            "Train loss 198 5.603466 Grad Norm 2.811731 4.40s/it\n",
            "Train loss 199 5.242976 Grad Norm 3.690259 4.90s/it\n",
            "Train loss 200 5.517758 Grad Norm 3.730336 4.50s/it\n",
            "Train loss 201 4.609179 Grad Norm 2.336696 4.04s/it\n",
            "Train loss 202 5.428163 Grad Norm 2.299603 4.51s/it\n",
            "Train loss 203 5.473228 Grad Norm 3.016326 4.35s/it\n",
            "Train loss 204 4.202480 Grad Norm 3.213367 4.80s/it\n",
            "Train loss 205 6.164336 Grad Norm 2.966907 4.24s/it\n",
            "Train loss 206 4.841514 Grad Norm 2.648316 4.59s/it\n",
            "Train loss 207 4.519169 Grad Norm 2.169306 5.14s/it\n",
            "Train loss 208 4.839003 Grad Norm 2.677819 3.93s/it\n",
            "Train loss 209 4.592927 Grad Norm 4.670328 4.30s/it\n",
            "Train loss 210 6.344254 Grad Norm 4.501374 3.62s/it\n",
            "Train loss 211 6.122136 Grad Norm 3.171858 4.71s/it\n",
            "Train loss 212 5.501249 Grad Norm 2.977432 3.90s/it\n",
            "Train loss 213 3.611905 Grad Norm 4.235018 5.20s/it\n",
            "Train loss 214 5.429744 Grad Norm 3.967887 5.04s/it\n",
            "Train loss 215 4.982615 Grad Norm 3.652545 4.32s/it\n",
            "Train loss 216 4.888543 Grad Norm 3.172146 4.97s/it\n",
            "Train loss 217 4.735352 Grad Norm 2.687857 4.50s/it\n",
            "Train loss 218 4.968573 Grad Norm 5.650657 4.65s/it\n",
            "Train loss 219 4.664531 Grad Norm 4.651947 4.97s/it\n",
            "Train loss 220 5.006732 Grad Norm 3.025686 4.50s/it\n",
            "Train loss 221 5.197303 Grad Norm 2.499822 4.92s/it\n",
            "Train loss 222 4.754565 Grad Norm 4.024917 4.87s/it\n",
            "Train loss 223 4.002493 Grad Norm 1.670727 4.63s/it\n",
            "Train loss 224 4.951993 Grad Norm 2.685151 4.32s/it\n",
            "Train loss 225 5.022149 Grad Norm 2.400678 4.62s/it\n",
            "Train loss 226 4.213733 Grad Norm 2.406067 5.01s/it\n",
            "Train loss 227 5.938399 Grad Norm 4.609440 4.67s/it\n",
            "Train loss 228 4.420814 Grad Norm 2.719190 4.76s/it\n",
            "Train loss 229 5.696079 Grad Norm 1.676092 4.32s/it\n",
            "Train loss 230 5.348562 Grad Norm 5.087878 3.63s/it\n",
            "Train loss 231 5.398304 Grad Norm 3.831350 4.02s/it\n",
            "Train loss 232 4.423652 Grad Norm 2.687749 4.38s/it\n",
            "Train loss 233 5.809381 Grad Norm 5.490822 4.52s/it\n",
            "Train loss 234 4.479473 Grad Norm 6.007125 4.54s/it\n",
            "Train loss 235 5.138288 Grad Norm 4.204035 5.03s/it\n",
            "Train loss 236 4.803866 Grad Norm 2.070158 4.48s/it\n",
            "Train loss 237 4.804266 Grad Norm 4.203957 4.81s/it\n",
            "Train loss 238 5.168100 Grad Norm 2.323790 4.76s/it\n",
            "Train loss 239 3.582392 Grad Norm 2.734485 5.15s/it\n",
            "Train loss 240 5.509177 Grad Norm 3.574953 4.93s/it\n",
            "Train loss 241 5.463502 Grad Norm 3.470921 4.66s/it\n",
            "Train loss 242 4.598264 Grad Norm 2.268328 4.51s/it\n",
            "Train loss 243 5.295853 Grad Norm 5.129261 4.42s/it\n",
            "Train loss 244 4.365581 Grad Norm 5.492375 2.68s/it\n",
            "Train loss 245 4.369367 Grad Norm 4.396945 4.20s/it\n",
            "Train loss 246 4.574399 Grad Norm 4.656702 4.58s/it\n",
            "Train loss 247 4.845979 Grad Norm 2.159299 3.99s/it\n",
            "Train loss 248 4.047827 Grad Norm 2.472294 4.58s/it\n",
            "Train loss 249 5.046161 Grad Norm 2.727641 5.12s/it\n",
            "Train loss 250 4.985717 Grad Norm 3.315707 3.45s/it\n",
            "Train loss 251 4.810888 Grad Norm 2.719636 4.76s/it\n",
            "Train loss 252 4.745229 Grad Norm 1.927686 4.07s/it\n",
            "Train loss 253 4.083031 Grad Norm 2.313722 4.31s/it\n",
            "Train loss 254 5.381157 Grad Norm 2.595301 4.05s/it\n",
            "Train loss 255 5.019095 Grad Norm 1.635255 4.80s/it\n",
            "Train loss 256 4.511080 Grad Norm 3.939979 3.48s/it\n",
            "Train loss 257 4.519916 Grad Norm 2.309744 4.73s/it\n",
            "Train loss 258 5.562957 Grad Norm 2.292464 4.25s/it\n",
            "Train loss 259 4.921963 Grad Norm 2.144875 3.69s/it\n",
            "Train loss 260 4.048506 Grad Norm 2.436802 4.24s/it\n",
            "Train loss 261 5.061656 Grad Norm 2.807224 5.01s/it\n",
            "Train loss 262 5.387001 Grad Norm 3.313530 3.26s/it\n",
            "Train loss 263 4.755394 Grad Norm 2.485047 4.89s/it\n",
            "Train loss 264 4.843047 Grad Norm 1.805654 4.43s/it\n",
            "Train loss 265 4.846633 Grad Norm 3.687566 4.23s/it\n",
            "Train loss 266 4.860823 Grad Norm 2.223616 3.88s/it\n",
            "Train loss 267 5.028684 Grad Norm 3.545756 5.08s/it\n",
            "Train loss 268 4.574900 Grad Norm 2.365014 4.19s/it\n",
            "Train loss 269 4.012334 Grad Norm 1.483907 4.75s/it\n",
            "Train loss 270 4.064084 Grad Norm 1.509191 4.44s/it\n",
            "Train loss 271 4.286277 Grad Norm 1.985770 4.93s/it\n",
            "Train loss 272 3.782466 Grad Norm 2.908365 3.56s/it\n",
            "Train loss 273 5.059715 Grad Norm 4.046486 3.92s/it\n",
            "Train loss 274 5.524717 Grad Norm 2.529906 4.94s/it\n",
            "Train loss 275 3.997861 Grad Norm 2.190547 3.98s/it\n",
            "Train loss 276 4.751331 Grad Norm 2.231089 5.03s/it\n",
            "Train loss 277 5.108747 Grad Norm 3.647046 4.53s/it\n",
            "Train loss 278 5.288033 Grad Norm 3.435241 4.60s/it\n",
            "Train loss 279 4.141299 Grad Norm 1.943604 4.75s/it\n",
            "Train loss 280 4.592009 Grad Norm 1.834303 3.29s/it\n",
            "Train loss 281 4.787544 Grad Norm 2.809883 4.86s/it\n",
            "Train loss 282 4.165594 Grad Norm 2.087254 4.14s/it\n",
            "Train loss 283 5.604893 Grad Norm 3.684100 3.97s/it\n",
            "Train loss 284 5.235474 Grad Norm 3.095168 4.55s/it\n",
            "Train loss 285 4.940039 Grad Norm 3.135237 4.26s/it\n",
            "Train loss 286 4.566453 Grad Norm 2.367815 5.23s/it\n",
            "Train loss 287 5.600600 Grad Norm 2.117762 4.25s/it\n",
            "Train loss 288 4.525935 Grad Norm 2.025019 5.05s/it\n",
            "Train loss 289 3.966009 Grad Norm 2.359128 3.84s/it\n",
            "Train loss 290 4.776719 Grad Norm 2.865142 3.79s/it\n",
            "Train loss 291 4.725690 Grad Norm 2.741602 4.75s/it\n",
            "Train loss 292 4.208666 Grad Norm 2.209458 4.17s/it\n",
            "Train loss 293 5.246902 Grad Norm 2.762529 4.96s/it\n",
            "Train loss 294 4.790816 Grad Norm 2.238822 5.16s/it\n",
            "Train loss 295 5.016226 Grad Norm 1.693460 4.20s/it\n",
            "Train loss 296 4.909623 Grad Norm 2.485333 5.15s/it\n",
            "Train loss 297 4.213570 Grad Norm 2.430367 4.84s/it\n",
            "Train loss 298 4.845766 Grad Norm 2.317820 3.57s/it\n",
            "Train loss 299 4.165431 Grad Norm 2.218496 3.12s/it\n",
            "Train loss 300 4.698305 Grad Norm 2.414369 3.88s/it\n",
            "Train loss 301 4.694235 Grad Norm 2.804657 5.17s/it\n",
            "Train loss 302 3.654266 Grad Norm 2.010609 4.87s/it\n",
            "Train loss 303 5.074563 Grad Norm 2.256260 5.21s/it\n",
            "Train loss 304 4.034672 Grad Norm 1.769209 5.12s/it\n",
            "Train loss 305 3.945857 Grad Norm 2.331246 4.91s/it\n",
            "Train loss 306 4.332537 Grad Norm 2.150549 4.91s/it\n",
            "Train loss 307 4.602465 Grad Norm 3.415010 3.87s/it\n",
            "Train loss 308 4.236024 Grad Norm 2.342314 5.07s/it\n",
            "Train loss 309 4.364902 Grad Norm 2.298219 4.78s/it\n",
            "Train loss 310 4.126187 Grad Norm 2.081573 3.64s/it\n",
            "Train loss 311 5.031242 Grad Norm 1.785275 4.45s/it\n",
            "Train loss 312 5.046273 Grad Norm 3.847225 4.76s/it\n",
            "Train loss 313 4.165187 Grad Norm 3.259617 4.63s/it\n",
            "Train loss 314 4.098181 Grad Norm 2.466699 4.87s/it\n",
            "Train loss 315 4.704124 Grad Norm 4.472497 4.43s/it\n",
            "Train loss 316 4.363500 Grad Norm 2.738334 3.40s/it\n",
            "Train loss 317 4.179873 Grad Norm 2.099525 4.29s/it\n",
            "Train loss 318 5.127634 Grad Norm 2.130598 4.67s/it\n",
            "Train loss 319 4.844055 Grad Norm 2.192360 4.60s/it\n",
            "Train loss 320 3.447832 Grad Norm 2.178074 4.29s/it\n",
            "Train loss 321 4.427372 Grad Norm 3.234909 4.40s/it\n",
            "Train loss 322 5.277299 Grad Norm 2.326133 4.77s/it\n",
            "Train loss 323 4.920017 Grad Norm 2.680753 5.19s/it\n",
            "Train loss 324 5.486397 Grad Norm 3.258190 5.08s/it\n",
            "Train loss 325 3.594406 Grad Norm 4.855622 5.16s/it\n",
            "Train loss 326 4.456501 Grad Norm 2.257720 2.65s/it\n",
            "Train loss 327 3.750328 Grad Norm 1.840257 4.98s/it\n",
            "Train loss 328 3.927621 Grad Norm 2.805763 4.63s/it\n",
            "Train loss 329 3.746240 Grad Norm 2.366468 4.91s/it\n",
            "Train loss 330 3.963267 Grad Norm 1.889148 3.46s/it\n",
            "Train loss 331 4.272395 Grad Norm 3.263193 3.62s/it\n",
            "Train loss 332 4.990001 Grad Norm 5.495007 5.07s/it\n",
            "Train loss 333 4.380429 Grad Norm 1.879406 5.15s/it\n",
            "Train loss 334 4.808352 Grad Norm 4.213294 5.05s/it\n",
            "Train loss 335 3.771131 Grad Norm 3.325965 4.99s/it\n",
            "Train loss 336 3.808392 Grad Norm 2.926510 4.48s/it\n",
            "Train loss 337 3.582307 Grad Norm 2.324590 4.49s/it\n",
            "Train loss 338 4.578459 Grad Norm 3.424897 4.90s/it\n",
            "Train loss 339 4.733035 Grad Norm 2.593918 4.88s/it\n",
            "Train loss 340 4.171882 Grad Norm 1.833079 4.96s/it\n",
            "Train loss 341 4.085982 Grad Norm 2.411138 4.88s/it\n",
            "Train loss 342 5.119249 Grad Norm 3.362862 4.55s/it\n",
            "Train loss 343 3.207695 Grad Norm 2.418787 4.96s/it\n",
            "Train loss 344 4.404643 Grad Norm 2.438119 4.75s/it\n",
            "Train loss 345 4.199846 Grad Norm 2.031637 4.25s/it\n",
            "Train loss 346 4.096679 Grad Norm 1.852692 4.83s/it\n",
            "Train loss 347 3.711484 Grad Norm 1.402484 4.82s/it\n",
            "Train loss 348 4.741255 Grad Norm 3.190528 4.55s/it\n",
            "Train loss 349 4.466525 Grad Norm 1.826935 4.65s/it\n",
            "Train loss 350 4.326514 Grad Norm 1.471923 5.05s/it\n",
            "Train loss 351 4.580657 Grad Norm 2.128951 4.22s/it\n",
            "Train loss 352 4.185548 Grad Norm 2.694986 4.36s/it\n",
            "Train loss 353 5.189860 Grad Norm 2.645201 4.47s/it\n",
            "Train loss 354 4.519989 Grad Norm 3.559419 4.88s/it\n",
            "Train loss 355 3.076600 Grad Norm 1.317869 3.61s/it\n",
            "Train loss 356 3.049892 Grad Norm 2.058280 4.07s/it\n",
            "Train loss 357 3.902486 Grad Norm 2.101585 4.93s/it\n",
            "Train loss 358 4.747757 Grad Norm 3.622046 4.19s/it\n",
            "Train loss 359 4.709518 Grad Norm 4.283039 4.56s/it\n",
            "Train loss 360 5.160427 Grad Norm 3.868208 4.04s/it\n",
            "Train loss 361 4.342185 Grad Norm 1.953829 4.91s/it\n",
            "Train loss 362 4.099227 Grad Norm 2.931156 5.01s/it\n",
            "Train loss 363 4.850863 Grad Norm 4.903281 3.97s/it\n",
            "Train loss 364 3.906347 Grad Norm 2.262486 4.99s/it\n",
            "Train loss 365 4.256482 Grad Norm 1.880951 4.72s/it\n",
            "Train loss 366 3.187201 Grad Norm 2.944412 4.90s/it\n",
            "Train loss 367 4.475883 Grad Norm 3.311190 3.57s/it\n",
            "Train loss 368 4.667299 Grad Norm 2.594840 3.85s/it\n",
            "Train loss 369 4.681089 Grad Norm 2.350461 4.72s/it\n",
            "Train loss 370 4.428678 Grad Norm 2.861212 4.73s/it\n",
            "Train loss 371 4.107143 Grad Norm 2.623620 4.83s/it\n",
            "Train loss 372 5.043904 Grad Norm 4.152089 4.13s/it\n",
            "Train loss 373 4.420777 Grad Norm 2.843781 3.69s/it\n",
            "Train loss 374 4.376390 Grad Norm 2.610466 5.06s/it\n",
            "Train loss 375 3.502602 Grad Norm 3.764272 4.82s/it\n",
            "Train loss 376 3.975061 Grad Norm 2.514396 4.34s/it\n",
            "Train loss 377 3.846085 Grad Norm 1.485955 4.65s/it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvREI3rDraKu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}